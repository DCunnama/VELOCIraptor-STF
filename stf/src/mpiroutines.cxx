/*! \file mpiroutines.cxx
 *  \brief this file contains routines used with MPI compilation. 

    MPI routines generally pertain to domain decomposition or to specific MPI tasks that determine what needs to be broadcast between various threads.

 */

#ifdef USEMPI

//-- For MPI

#include "stf.h"

/// \name Domain decomposition routines and io routines to place particles correctly in local mpi data space
//@{

/*! 
    Determine the domain decomposition.\n
    Here the domains are constructured in data units
    only ThisTask==0 should call this routine. It is tricky to get appropriate load balancing and correct number of particles per processor.\n

    I could use recursive binary splitting like kd-tree along most spread axis till have appropriate number of volumes corresponding 
    to number of processors.

    NOTE: assume that cannot store data so position information is read Nsplit times to determine boundaries of subvolumes
    could also randomly subsample system and produce tree from that 
    should store for each processor the node structure generated by the domain decomposition
    what I could do is read file twice, one to get extent and other to calculate entropy then decompose
    along some primary axis, then choose orthogonal axis, keep iterating till have appropriate number of subvolumes
    then store the boundaries of the subvolume. this means I don't store data but get at least reasonable domain decomposition

    NOTE: pkdgrav uses orthoganl recursive bisection along with kd-tree, gadget-2 uses peno-hilbert curve to map particles and oct-trees 
    the question with either method is guaranteeing load balancing. for ORB achieved by splitting (sub)volume along a dimension (say one with largest spread or max entropy)
    such that either side of the cut has approximately the same number of particles (ie: median splitting). But for both cases, load balancing requires particle information
    so I must load the system then move particles about to ensure load balancing.

    Main thing first is get the dimensional extent of the system.
    then I could get initial splitting just using mid point between boundaries along each dimension.
    once have that initial splitting just load data then start shifting data around.
*/

///determine the initial domains, ie: bisection distance mpi_dxsplit, which is used to determien what processor a particle is assigned to
///here the domains are constructured in data units
void MPIInitialDomainDecomposition(){
    int temp,Nsplit=log((float)NProcs)/log(2.0);
    Double_t posfirst[3],deltax[3];
    int isplit;

    if (ThisTask==0) {
    /*
    //expand limits by a small amount
    for (int j=0;j<3;j++) {
        Double_t dx=0.001*(mpi_xlim[j][1]-mpi_xlim[j][0]);
        mpi_xlim[j][0]-=dx;mpi_xlim[j][1]+=dx;
    }
    for (int j=0;j<3;j++) deltax[j]=(mpi_xlim[j][1]-mpi_xlim[j][0]);
    //determine order of spliting
    PriorityQueue *pq=new PriorityQueue(3);
    for (int j=0;j<3;j++) pq->Push(j,deltax[j]);
    for (int j=0;j<3;j++) {mpi_ideltax[j]=pq->TopQueue();pq->Pop();}
    delete pq;

    //now split halo along orthogonal axes
    //first determine number of bisections along each axis, where bisection point is just mid point in dimension, not mid point in particle number
    isplit=0;
    for (int j=0;j<3;j++) mpi_nxsplit[j]=0;
    for (int l=0;l<Nsplit;l++) {
        mpi_nxsplit[mpi_ideltax[isplit++]]++;
        if (isplit==3) isplit=0;
    }
    for (int j=0;j<3;j++) mpi_nxsplit[j]=pow(2.0,mpi_nxsplit[j]);
    for (int j=0;j<3;j++) mpi_dxsplit[mpi_ideltax[j]]=deltax[mpi_ideltax[j]]/mpi_nxsplit[mpi_ideltax[j]];

    //set boundaries spatial boundaries of a given threads domain
    for (int k=0;k<mpi_nxsplit[mpi_ideltax[2]];k++) {
        for (int j=0;j<mpi_nxsplit[mpi_ideltax[1]];j++) {
            for (int i=0;i<mpi_nxsplit[mpi_ideltax[0]];i++) {
                int ix=mpi_ideltax[0],iy=mpi_ideltax[1],iz=mpi_ideltax[2];
                int mpitasknum=i+j*mpi_nxsplit[ix]+k*(mpi_nxsplit[ix]*mpi_nxsplit[iy]);
                mpi_domain[mpitasknum].bnd[ix][0]=mpi_xlim[ix][0]+i*mpi_dxsplit[ix];mpi_domain[mpitasknum].bnd[ix][1]=mpi_domain[mpitasknum].bnd[ix][0]+mpi_dxsplit[ix];
                mpi_domain[mpitasknum].bnd[iy][0]=mpi_xlim[iy][0]+j*mpi_dxsplit[iy];mpi_domain[mpitasknum].bnd[iy][1]=mpi_domain[mpitasknum].bnd[iy][0]+mpi_dxsplit[iy];
                mpi_domain[mpitasknum].bnd[iz][0]=mpi_xlim[iz][0]+k*mpi_dxsplit[iz];mpi_domain[mpitasknum].bnd[iz][1]=mpi_domain[mpitasknum].bnd[iz][0]+mpi_dxsplit[iz];
            }
        }
    }
    */
    cout<<"MPI Domains are: "<<endl;
    for (int j=0;j<NProcs;j++) {
        cout<<"ThisTask= "<<j<<" :: ";
        cout.precision(10);for (int k=0;k<3;k++) cout<<k<<" "<<mpi_domain[j].bnd[k][0]<<" "<<mpi_domain[j].bnd[k][1]<<" | ";cout<<endl;
    }
    }
    //broadcast data
    MPI_Bcast(mpi_domain, NProcs*sizeof(MPI_Domain), MPI_BYTE, 0, MPI_COMM_WORLD);

}


void MPINumInDomain(Options &opt)
{
    if(opt.inputtype==IOTIPSY) MPINumInDomainTipsy(opt);
    else if (opt.inputtype==IOGADGET) MPINumInDomainGadget(opt);
    else if (opt.inputtype==IORAMSES) MPINumInDomainRAMSES(opt);
#ifdef USEHDF
    else if (opt.inputtype==IOHDF) MPINumInDomainHDF(opt);
#endif
}

///adjust the domain boundaries to code units
void MPIAdjustDomain(Options opt){
    Double_t aadjust, lscale;
    if (opt.comove) aadjust=1.0;
    else aadjust=opt.a;
    lscale=opt.L/opt.h*aadjust;
    for (int j=0;j<NProcs;j++) for (int k=0;k<3;k++) {mpi_domain[j].bnd[k][0]*=lscale;mpi_domain[j].bnd[k][1]*=lscale;}
}

///given a position and a mpi thread domain information, determine which processor a particle is assigned to
int MPIGetParticlesProcessor(Double_t x,Double_t y, Double_t z){
    for (int j=0;j<NProcs;j++)
        if( (mpi_domain[j].bnd[0][0]<=x) && (mpi_domain[j].bnd[0][1]>x)&&
            (mpi_domain[j].bnd[1][0]<=y) && (mpi_domain[j].bnd[1][1]>y)&&
            (mpi_domain[j].bnd[2][0]<=z) && (mpi_domain[j].bnd[2][1]>z) )
            return j;
}
//@}

/// \name routines which check to see if some search region overlaps with local mpi domain
//@{
///search if some region is in the local mpi domain
int MPIInDomain(Double_t xsearch[3][2], Double_t bnd[3][2]){
    Double_t xsearchp[3][2];
    if (!((bnd[0][1] < xsearch[0][0]) || (bnd[0][0] > xsearch[0][1]) ||
        (bnd[1][1] < xsearch[1][0]) || (bnd[1][0] > xsearch[1][1]) ||
        (bnd[2][1] < xsearch[2][0]) || (bnd[2][0] > xsearch[2][1])))
            return 1;
    else {
        if (mpi_period==0) return 0;
        else {
            for (int j=0;j<3;j++) {xsearchp[j][0]=xsearch[j][0];xsearchp[j][1]=xsearch[j][1];}
            for (int j=0;j<3;j++) {
                if (!((bnd[j][1] < xsearch[j][0]+mpi_period) || (bnd[j][0] > xsearch[j][1]+mpi_period))) {xsearchp[j][0]+=mpi_period;xsearchp[j][1]+=mpi_period;}
                else if (!((bnd[j][1] < xsearch[j][0]-mpi_period) || (bnd[j][0] > xsearch[j][1]-mpi_period))) {xsearchp[j][0]-=mpi_period;xsearchp[j][1]-=mpi_period;}
            }
            if (!((bnd[0][1] < xsearchp[0][0]) || (bnd[0][0] > xsearchp[0][1]) ||
            (bnd[1][1] < xsearchp[1][0]) || (bnd[1][0] > xsearchp[1][1]) ||
            (bnd[2][1] < xsearchp[2][0]) || (bnd[2][0] > xsearchp[2][1])))
                return 1;
            else return 0;
        }
    }
}

///\todo clean up memory allocation in these functions, no need to keep allocating xsearch,xsearchp,numoverlap,etc
/// Determine if a particle needs to be exported to another mpi domain based on a physical search radius
int MPISearchForOverlap(Particle &Part, Double_t &rdist){
    Double_t xsearch[3][2];
    Double_t xsearchp[7][3][2];//used to store periodic reflections
    int numoverlap=0,numreflecs=0,ireflec[3],numreflecchoice=0;
    int indomain;
    int j,k;

    for (k=0;k<3;k++) {xsearch[k][0]=Part.GetPosition(k)-rdist;xsearch[k][1]=Part.GetPosition(k)+rdist;}
    for (j=0;j<NProcs;j++) {
        if (j!=ThisTask) {
            //determine if search region is not outside of this processors domain
            if(!((mpi_domain[j].bnd[0][1] < xsearch[0][0]) || (mpi_domain[j].bnd[0][0] > xsearch[0][1]) ||
                (mpi_domain[j].bnd[1][1] < xsearch[1][0]) || (mpi_domain[j].bnd[1][0] > xsearch[1][1]) ||
                (mpi_domain[j].bnd[2][1] < xsearch[2][0]) || (mpi_domain[j].bnd[2][0] > xsearch[2][1])))
                numoverlap++;
        }
    }
    if (mpi_period!=0) {
        for (k=0;k<3;k++) if (xsearch[k][0]<0||xsearch[k][1]>mpi_period) ireflec[numreflecs++]=k;
        if (numreflecs==1)numreflecchoice=1;
        else if (numreflecs==2) numreflecchoice=3;
        else if (numreflecs==3) numreflecchoice=7;
        for (j=0;j<numreflecchoice;j++) for (k=0;k<3;k++) {xsearchp[j][k][0]=xsearch[k][0];xsearchp[j][k][1]=xsearch[k][1];}
        if (numreflecs==1) {
            if (xsearch[ireflec[0]][0]<0) {
                    xsearchp[0][ireflec[0]][0]=xsearch[ireflec[0]][0]+mpi_period;
                    xsearchp[0][ireflec[0]][1]=xsearch[ireflec[0]][1]+mpi_period;
            }
            else if (xsearch[ireflec[0]][1]>mpi_period) {
                    xsearchp[0][ireflec[0]][0]=xsearch[ireflec[0]][0]-mpi_period;
                    xsearchp[0][ireflec[0]][1]=xsearch[ireflec[0]][1]-mpi_period;
            }
        }
        else if (numreflecs==2) {
            k=0;j=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;j++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
        }
        else if (numreflecs==3) {
            j=0;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=1;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=2;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=1;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k=2;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            j++;k=0;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
            k++;
            if (xsearch[ireflec[k]][0]<0) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]+mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]+mpi_period;
            }
            else if (xsearch[ireflec[k]][1]>mpi_period) {
                    xsearchp[j][ireflec[k]][0]=xsearch[ireflec[k]][0]-mpi_period;
                    xsearchp[j][ireflec[k]][1]=xsearch[ireflec[k]][1]-mpi_period;
            }
        }

    for (j=0;j<NProcs;j++) for (k=0;k<numreflecchoice;k++){
        if (j!=ThisTask) {
            if(!((mpi_domain[j].bnd[0][1] < xsearchp[k][0][0]) || (mpi_domain[j].bnd[0][0] > xsearchp[k][0][1]) ||
            (mpi_domain[j].bnd[1][1] < xsearchp[k][1][0]) || (mpi_domain[j].bnd[1][0] > xsearchp[k][1][1]) ||
            (mpi_domain[j].bnd[2][1] < xsearchp[k][2][0]) || (mpi_domain[j].bnd[2][0] > xsearchp[k][2][1])))
            numoverlap++;
        }
    }
    }
    return numoverlap;
}
//@}

/// \name Routines involved in exporting particles
//@{ 
/*! Determine number of particles have a spatial linking length such that linking overlaps the domain of another processor 
*/
void MPIGetExportNum(const Int_t nbodies, Particle *&Part, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles 
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    NExport=nexport;//*(1.0+MPIExportFac);
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
}

/*! Determine which particles have a spatial linking length such that linking overlaps the domain of another processor store the necessary information to send that data
    and then send that information
*/
void MPIBuildParticleExportList(const Int_t nbodies, Particle *&Part, Int_t *&pfof, Int_t *&Len, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles 
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    //FoFDataIn[nexport].Part=Part[i];
                    FoFDataIn[nexport].Index = i;
                    FoFDataIn[nexport].Task = j;
                    FoFDataIn[nexport].iGroup = pfof[Part[i].GetID()];//set group id
                    FoFDataIn[nexport].iGroupTask = ThisTask;//and the task of the group
                    FoFDataIn[nexport].iLen = Len[i];
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    if (nexport>0) {
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number 
    qsort(FoFDataIn, nexport, sizeof(struct fofdata_in), fof_export_cmp);
    for (i=0;i<nexport;i++) PartDataIn[i] = Part[FoFDataIn[i].Index];
    }
    //then store the offset in the export particle data for the jth Task in order to send data. 
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    //now send the data.
    ///\todo In determination of particle export for FOF routines, eventually need to place a check for the communication buffer so that if exported number 
    ///is larger than the size of the buffer, iterate over the number exported
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];
    if (nexport>0||nimport>0) {
    for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;//bitwise XOR ensures that recvTask cycles around sendTask
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer 
                //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                //first send FOF data and then particle data
                MPI_Sendrecv(&FoFDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(struct fofdata_in), MPI_BYTE,
                    recvTask, TAG_FOF_A,
                    &FoFDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofdata_in),
                    MPI_BYTE, recvTask, TAG_FOF_A, MPI_COMM_WORLD, &status);
                MPI_Sendrecv(&PartDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(Particle), MPI_BYTE,
                    recvTask, TAG_FOF_B,
                    &PartDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(Particle),
                    MPI_BYTE, recvTask, TAG_FOF_B, MPI_COMM_WORLD, &status);
            }
        }
    }
    }
}

/*! like \ref MPIGetExportNum but number based on NN search, useful for reducing memory costs at the expense of cpu cycles
*/
void MPIGetNNExportNum(const Int_t nbodies, Particle *Part, Double_t *rdist, int istrucdenflag){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int indomain;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles 
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) 
    {
        if (istrucdenflag && Part[i].GetType()==0) continue;
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist[i];xsearch[k][1]=Part[i].GetPosition(k)+rdist[i];}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    NExport=nexport;
}

/*! like \ref MPIBuildParticleExportList but each particle has a different distance stored in rdist used to find nearest neighbours
*/
void MPIBuildParticleNNExportList(const Int_t nbodies, Particle *Part, Double_t *rdist, int istrucdenflag){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int indomain;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles 
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) 
    {
        if (istrucdenflag && Part[i].GetType()==0) continue;
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist[i];xsearch[k][1]=Part[i].GetPosition(k)+rdist[i];}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    //NNDataIn[nexport].Index=i;
                    NNDataIn[nexport].ToTask=j;
                    NNDataIn[nexport].FromTask=ThisTask;
                    NNDataIn[nexport].R2=rdist[i]*rdist[i];
                    //NNDataIn[nexport].V2=vdist2[i];
                    for (int k=0;k<3;k++) {
                        NNDataIn[nexport].Pos[k]=Part[i].GetPosition(k);
                        NNDataIn[nexport].Vel[k]=Part[i].GetVelocity(k);
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number 
    if (nexport>0) qsort(NNDataIn, nexport, sizeof(struct nndata_in), nn_export_cmp);

    //then store the offset in the export particle data for the jth Task in order to send data. 
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    //now send the data.
    ///\todo In determination of particle export, eventually need to place a check for the communication buffer so that if exported number 
    ///is larger than the size of the buffer, iterate over the number exported
    //if either sending or receiving then run this process
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];
    if (nexport>0||nimport>0) {
    for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer 
                //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                MPI_Sendrecv(&NNDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(struct nndata_in), MPI_BYTE,
                    recvTask, TAG_NN_A,
                    &NNDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct nndata_in),
                    MPI_BYTE, recvTask, TAG_NN_A, MPI_COMM_WORLD, &status);
            }
        }
    }
    }
    MPI_Barrier(MPI_COMM_WORLD);
}

/*! Mirror to \ref MPIGetNNExportNum, use exported particles, run ball search to find number of all local particles that need to be
    imported back to exported particle's thread so that a proper NN search can be made.
*/
void MPIGetNNImportNum(const Int_t nbodies, KDTree *tree, Particle *Part){
    Int_t i, j,nthreads,nexport=0,ncount;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Int_t oldnsend[NProcs*NProcs];
    Double_t xsearch[3][2];
    Int_t *nn=new Int_t[nbodies];
    Double_t *nnr2=new Double_t[nbodies];
    nthreads=1;
    Int_t sendTask,recvTask;
    MPI_Status status;
#ifdef USEOPENMP
#pragma omp parallel 
    {
            if (omp_get_thread_num()==0) nthreads=omp_get_num_threads();
    }
#endif
    for(j=0;j<NProcs;j++)
    {
        nbuffer[j]=0;
        for (int k=0;k<j;k++)nbuffer[j]+=mpi_nsend[ThisTask+k*NProcs];//offset on "receiver" end
    }

    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (j=0;j<NProcs;j++) {
#ifdef USEOPENMP
#pragma omp parallel default(shared) \
private(i)
{
#pragma omp for
#endif
            for (i=0;i<nbodies;i++) nn[i]=-1;
#ifdef USEOPENMP
}
#endif
        if (j!=ThisTask) {
            //search local list and tag all local particles that need to be exported back (or imported) to the exported particles thread 
            for (i=nbuffer[j];i<nbuffer[j]+mpi_nsend[ThisTask+j*NProcs];i++) {
                tree->SearchBallPos(NNDataGet[i].Pos, NNDataGet[i].R2, j, nn, nnr2);
            }
            for (i=0;i<nbodies;i++) {
                if (nn[i]!=-1) {
                    for (int k=0;k<3;k++) {
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    //must store old mpi nsend for accessing NNDataGet properly.
    for (j=0;j<NProcs;j++) for (int k=0;k<NProcs;k++) oldnsend[k+j*NProcs]=mpi_nsend[k+j*NProcs];
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;
    for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    NExport=nexport;
    for (j=0;j<NProcs;j++) for (int k=0;k<NProcs;k++) mpi_nsend[k+j*NProcs]=oldnsend[k+j*NProcs];
}

/*! Mirror to \ref MPIBuildParticleNNExportList, use exported particles, run ball search to find all local particles that need to be
    imported back to exported particle's thread so that a proper NN search can be made.
*/
Int_t MPIBuildParticleNNImportList(const Int_t nbodies, KDTree *tree, Particle *Part, int iallflag, int istrucdenflag){
    Int_t i, j,nthreads,nexport=0,ncount;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t *nn=new Int_t[nbodies];
    Double_t *nnr2=new Double_t[nbodies];
    nthreads=1;
    Int_t sendTask,recvTask;
    MPI_Status status;
#ifdef USEOPENMP
#pragma omp parallel 
    {
            if (omp_get_thread_num()==0) nthreads=omp_get_num_threads();
    }
#endif
    for(j=0;j<NProcs;j++)
    {
        nbuffer[j]=0;
        for (int k=0;k<j;k++)nbuffer[j]+=mpi_nsend[ThisTask+k*NProcs];//offset on "receiver" end
    }

    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (j=0;j<NProcs;j++) {
#ifdef USEOPENMP
#pragma omp parallel default(shared) \
private(i)
{
#pragma omp for
#endif
            for (i=0;i<nbodies;i++) nn[i]=-1;
#ifdef USEOPENMP
}
#endif
        if (j!=ThisTask) {
            //search local list and tag all local particles that need to be exported back (or imported) to the exported particles thread 
            for (i=nbuffer[j];i<nbuffer[j]+mpi_nsend[ThisTask+j*NProcs];i++) {
                tree->SearchBallPos(NNDataGet[i].Pos, NNDataGet[i].R2, j, nn, nnr2);
            }
            //if not spliting search so that only calculated velocity density function based on dark matter particles 
            //as fof search is all but a separate baryon search is choosen for substructure, then just export particles
            //that are in spaitial window
            if (iallflag) {
            for (i=0;i<nbodies;i++) {
                if (nn[i]!=-1) {
                    for (int k=0;k<3;k++) {
                        PartDataIn[nexport].SetPosition(k,Part[i].GetPosition(k));
                        PartDataIn[nexport].SetVelocity(k,Part[i].GetVelocity(k));
                    }
                    nexport++;
                    nsend_local[j]++;
                }
            }
            }
            //otherwise, check the particle type either == dark matter or if struct den is on then type set to group number if dark and negative group number if not
            else {
            for (i=0;i<nbodies;i++) {
                if (istrucdenflag && !(nn[i]!=-1 && Part[i].GetType()>0)) continue;
                else if (istrucdenflag==0 && !(nn[i]!=-1 && Part[i].GetType()==DARKTYPE)) continue;
                for (int k=0;k<3;k++) {
                    PartDataIn[nexport].SetPosition(k,Part[i].GetPosition(k));
                    PartDataIn[nexport].SetVelocity(k,Part[i].GetVelocity(k));
                }
                nexport++;
                nsend_local[j]++;
            }
            }
        }
    }
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number 
    //qsort(NNDataReturn, nexport, sizeof(struct nndata_in), nn_export_cmp);

    //then store the offset in the export particle data for the jth Task in order to send data. 
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    //now send the data.
    ///\todo In determination of particle export, eventually need to place a check for the communication buffer so that if exported number 
    ///is larger than the size of the buffer, iterate over the number exported
    //if (nexport>0) {
    for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer 
                //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                MPI_Sendrecv(&PartDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(Particle), MPI_BYTE,
                    recvTask, TAG_NN_B,
                    &PartDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(Particle),
                    MPI_BYTE, recvTask, TAG_NN_B, MPI_COMM_WORLD, &status);
            }
        }
    }
    //}
    ncount=0;for (int k=0;k<NProcs;k++)ncount+=mpi_nsend[ThisTask+k*NProcs];
    return ncount;
}

/*! Similar to \ref MPIBuildParticleExportList, however this is for associated baryon search where particles have been moved from original 
    mpi domains and their group id accessed through the id array and their stored id and length in numingroup
*/
void MPIBuildParticleExportBaryonSearchList(const Int_t nbodies, Particle *&Part, Int_t *&pfof, Int_t *ids, Int_t *numingroup, Double_t rdist){
    Int_t i, j,nthreads,nexport=0,nimport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Double_t xsearch[3][2];
    Int_t sendTask,recvTask;
    MPI_Status status;

    ///\todo would like to add openmp to this code. In particular, loop over nbodies but issue is nexport.
    ///This would either require making a FoFDataIn[nthreads][NExport] structure so that each omp thread
    ///can only access the appropriate memory and adjust nsend_local.\n
    ///\em Or outer loop is over threads, inner loop over nbodies and just have a idlist of size Nlocal that tags particles 
    ///which must be exported. Then its a much quicker follow up loop (no if statement) that stores the data
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    for (i=0;i<nbodies;i++) {
        for (int k=0;k<3;k++) {xsearch[k][0]=Part[i].GetPosition(k)-rdist;xsearch[k][1]=Part[i].GetPosition(k)+rdist;}
        for (j=0;j<NProcs;j++) {
            if (j!=ThisTask) {
                //determine if search region is not outside of this processors domain
                if(MPIInDomain(xsearch,mpi_domain[j].bnd))
                {
                    //FoFDataIn[nexport].Part=Part[i];
                    FoFDataIn[nexport].Index = i;
                    FoFDataIn[nexport].Task = j;
                    FoFDataIn[nexport].iGroup = pfof[ids[Part[i].GetID()]];//set group id
                    FoFDataIn[nexport].iGroupTask = ThisTask;//and the task of the group
                    FoFDataIn[nexport].iLen = numingroup[pfof[ids[Part[i].GetID()]]];
                    nexport++;
                    nsend_local[j]++;
                }
            }
        }
    }
    if (nexport>0) {
    //sort the export data such that all particles to be passed to thread j are together in ascending thread number 
    qsort(FoFDataIn, nexport, sizeof(struct fofdata_in), fof_export_cmp);
    for (i=0;i<nexport;i++) PartDataIn[i] = Part[FoFDataIn[i].Index];
    }
    //then store the offset in the export particle data for the jth Task in order to send data. 
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    //and then gather the number of particles to be sent from mpi thread m to mpi thread n in the mpi_nsend[NProcs*NProcs] array via [n+m*NProcs]
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    NImport=0;for (j=0;j<NProcs;j++)NImport+=mpi_nsend[ThisTask+j*NProcs];
    //now send the data.
    ///\todo In determination of particle export for FOF routines, eventually need to place a check for the communication buffer so that if exported number 
    ///is larger than the size of the buffer, iterate over the number exported
    for (j=0;j<NProcs;j++)nimport+=mpi_nsend[ThisTask+j*NProcs];
    if (nexport>0||nimport>0) {
    for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;//bitwise XOR ensures that recvTask cycles around sendTask
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer 
                //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                //first send FOF data and then particle data
                MPI_Sendrecv(&FoFDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(struct fofdata_in), MPI_BYTE,
                    recvTask, TAG_FOF_A,
                    &FoFDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofdata_in),
                    MPI_BYTE, recvTask, TAG_FOF_A, MPI_COMM_WORLD, &status);
                MPI_Sendrecv(&PartDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(Particle), MPI_BYTE,
                    recvTask, TAG_FOF_B,
                    &PartDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(Particle),
                    MPI_BYTE, recvTask, TAG_FOF_B, MPI_COMM_WORLD, &status);
            }
        }
    }
    }
}

//@}

/// \name FOF related mpi routines 
//@{
///Set fof task id of particle
void MPISetTaskID(const Int_t nbodies){
    for (Int_t i=0;i<nbodies;i++) mpi_foftask[i]=ThisTask;
}

///Offset pfof array based so that local group numbers do not overlap
///\todo alter to that this now ranks threads so that group ids are larger if thread has more particles. This ensures that mpi threads
///send particles to thread with the fewest particles when linking across mpi domains during a FOF search
void MPIAdjustLocalGroupIDs(const Int_t nbodies, Int_t *pfof){
    PriorityQueue *pq=new PriorityQueue(NProcs);
    for (int j=0;j<NProcs;j++) pq->Push(j,mpi_nlocal[j]);
    Int_t rankorder[NProcs];
    for (int j=0;j<NProcs;j++) {rankorder[NProcs-1-j]=pq->TopQueue();pq->Pop();}
    Int_t offset=0;
    for (int j=0;j<NProcs;j++){if(rankorder[j]==ThisTask) break; offset+=mpi_nlocal[rankorder[j]];}
    //Int_t offset=nbodies*ThisTask;
    for (Int_t i=0;i<nbodies;i++) if (pfof[i]>0) pfof[i]+=offset;
    mpi_maxgid=0;
    for (int j=0;j<NProcs;j++){mpi_maxgid+=mpi_nlocal[rankorder[j]];}
    mpi_gidoffset=0;
    for (int j=0;j<NProcs;j++){if(rankorder[j]==ThisTask) break; mpi_gidoffset+=mpi_ngroups[rankorder[j]];}
}

//My idea is this for doing the stiching. First generate export list of particle data and another seperate data structure for the FOF data
//Next, when examining local search using export particles (since iGroup=0 is unlinked) if a export particle current iGroup is larger
//Then adjust the local particle and all members of its group so long as its group is NOT group zero. Calculate the number of new links
//and determine the total number of new links across all mpi threads.
//If that number is not zero, then groups have been found that are across processor domains.
//One has to iterate the check across the domains till no more new links have been found. That is must update the export particles Group ids
//then begin the check anew. 

//Couple of key things to think about are, one I really shouldn't have to run the check again to find the particles that meet the conditions across
//threads since that has NOT changed. must figure out a way to store relevant particles. Otherwise, continuously checking, seems a waste of cpu cycles. 
//second I must pass along head, tail, next, and length information, maybe by using a plist structure so that it is easy to alter the particles locally to new group id
//Also must determine optimal way of setting which processor the group should end up on. Best way might be to use the length of the group locally since
//that would minimize the broadcasts.

/*! Determine which particles have a spatial linking length such that linking overlaps the domain of another processor store the necessary information to send that data
    and then send that information
*/
void MPIUpdateExportList(const Int_t nbodies, Particle *&Part, Int_t *&pfof, Int_t *&Len){
    Int_t i, j,nthreads,nexport;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Int_t sendTask,recvTask;
    MPI_Status status;

    nexport=0;
    for (j=0;j<NProcs;j++) {nexport+=mpi_nsend[j+ThisTask*NProcs];nsend_local[j]=mpi_nsend[j+ThisTask*NProcs];}
    for(j = 1, noffset[0] = 0; j < NProcs; j++) noffset[j]=noffset[j-1] + nsend_local[j-1];
    for (i=0;i<nexport;i++) {
        FoFDataIn[i].iGroup = pfof[Part[FoFDataIn[i].Index].GetID()];
        FoFDataIn[i].iGroupTask=mpi_foftask[Part[FoFDataIn[i].Index].GetID()];
        FoFDataIn[i].iLen=Len[FoFDataIn[i].Index];
    }
    for(j=0;j<NProcs;j++)//for(j=1;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;//bitwise XOR ensures that recvTask cycles around sendTask
            nbuffer[recvTask]=0;
            for (int k=0;k<recvTask;k++)nbuffer[recvTask]+=mpi_nsend[ThisTask+k*NProcs];//offset on local receiving buffer
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking point-to-point send and receive. Here must determine the appropriate offset point in the local export buffer 
                //for sending data and also the local appropriate offset in the local the receive buffer for information sent from the local receiving buffer
                MPI_Sendrecv(&FoFDataIn[noffset[recvTask]],
                    nsend_local[recvTask] * sizeof(struct fofdata_in), MPI_BYTE,
                    recvTask, TAG_FOF_A,
                    &FoFDataGet[nbuffer[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofdata_in),
                    MPI_BYTE, recvTask, TAG_FOF_A, MPI_COMM_WORLD, &status);
            }
        }
    }
}

/*! This routine searches the local particle list using the positions of the exported particles to see if any local particles 
    met the linking criterion and any other FOF criteria of said exported particle. If that is the case, then the group id of the local particle
    and all other particles that belong to the same group are adjusted if the group id of the exported particle is smaller. This routine returns 
    the number of links found between the local particles and all other exported particles from all other mpi domains.
    \todo need to update lengths if strucden flag used to limit particles for which real velocity density calculated
*/
Int_t MPILinkAcross(const Int_t nbodies, KDTree *tree, Particle *&Part, Int_t *&pfof, Int_t *&Len, Int_t *&Head, Int_t *&Next, Double_t rdist2){
    Int_t i,j,k;
    Int_t links=0;
    Int_t nbuffer[NProcs];
    Int_t *nn=new Int_t[nbodies];
    Int_t nt;
    Coordinate x;
    for (i=0;i<NImport;i++) {
        for (j=0;j<3;j++) x[j]=PartDataGet[i].GetPosition(j);
        nt=tree->SearchBallPosTagged(x, rdist2, nn);
        for (Int_t ii=0;ii<nt;ii++) {
            k=nn[ii];
            if (FoFDataGet[i].iGroup==0)
            {
                if (pfof[Part[Head[k]].GetID()]==0&&Part[Head[k]].GetPID() > PartDataGet[i].GetPID()) {
                    pfof[Part[k].GetID()]=mpi_maxgid+mpi_gidoffset;///some unique identifier based on this task
                    mpi_gidoffset++;//increase unique identifier
                    Len[k]=1;
                    mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                    links++;
                }
            }
            else {
            if (pfof[Part[Head[k]].GetID()]>0)  {
                if(pfof[Part[Head[k]].GetID()] > FoFDataGet[i].iGroup) {
                    Int_t ss = Head[k];
                    Int_t oldlen=Len[k];
                    do{
                        pfof[Part[ss].GetID()]=FoFDataGet[i].iGroup;
                        mpi_foftask[Part[ss].GetID()]=FoFDataGet[i].iGroupTask;
                        Len[ss]=FoFDataGet[i].iLen+oldlen;
                    }while((ss = Next[ss]) >= 0);
                    FoFDataGet[i].iLen+=oldlen;
                    ss = Head[k];
                    links++;
                }
            }
            else {
                pfof[Part[k].GetID()]=FoFDataGet[i].iGroup;
                Len[k]=FoFDataGet[i].iLen;
                mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                FoFDataGet[i].iLen+=1;
                links++;
            }
            }
        }
    }
    return links;
}
///link particles belonging to the same group across mpi domains using comparison function
Int_t MPILinkAcross(const Int_t nbodies, KDTree *tree, Particle *&Part, Int_t *&pfof, Int_t *&Len, Int_t *&Head, Int_t *&Next, Double_t rdist2, FOFcompfunc &cmp, Double_t *params){
    Int_t i,j,k;
    Int_t links=0;
    Int_t nbuffer[NProcs];
    Int_t *nn=new Int_t[nbodies];
    Int_t nt;
    for (i=0;i<NImport;i++) {
        nt=tree->SearchCriterionTagged(PartDataGet[i], cmp, params, nn);
        for (Int_t ii=0;ii<nt;ii++) {
            k=nn[ii];
            if (FoFDataGet[i].iGroup==0)
            {
                if (pfof[Part[Head[k]].GetID()]==0&&Part[Head[k]].GetPID() > PartDataGet[i].GetPID()) {
                    pfof[Part[k].GetID()]=mpi_maxgid+mpi_gidoffset;///some unique identifier based on this task
                    mpi_gidoffset++;//increase unique identifier
                    Len[k]=1;
                    mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                    links++;
                }
            }
            else {
            if (pfof[Part[Head[k]].GetID()]>0)  {
                if(pfof[Part[Head[k]].GetID()] > FoFDataGet[i].iGroup) {
                    Int_t ss = Head[k];
                    do{
                        pfof[Part[ss].GetID()]=FoFDataGet[i].iGroup;
                        mpi_foftask[Part[ss].GetID()]=FoFDataGet[i].iGroupTask;
                        Len[ss]=FoFDataGet[i].iLen;
                    }while((ss = Next[ss]) >= 0);
                    ss = Head[k];
                    links++;
                }
            }
            else {
                pfof[Part[k].GetID()]=FoFDataGet[i].iGroup;
                Len[k]=FoFDataGet[i].iLen;
                mpi_foftask[Part[k].GetID()]=FoFDataGet[i].iGroupTask;
                links++;
            }
            }
        }
    }
    return links;
}

/*! 
    Group particles belong to a group to a particular mpi thread so that locally easy to determine 
    the maximum group size and reoder the group ids according to descending group size.
    return the new local number of particles
*/
Int_t MPIGroupExchange(const Int_t nbodies, Particle *&Part, Int_t *&pfof){
    Int_t i, j,nthreads,nexport,nimport,nlocal,n;
    Int_t nsend_local[NProcs],noffset_import[NProcs],noffset_export[NProcs],nbuffer[NProcs];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int task;
    FoFGroupDataExport=NULL;
    FoFGroupDataLocal=NULL;
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    //first determine how big a local array is needed to store linked particles and broadcast information to create new nsend array
    nlocal=0;
    for (i=0;i<nbodies;i++) {
        if (mpi_foftask[i]!=ThisTask) 
            nsend_local[mpi_foftask[i]]++;
    }
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    nexport=nimport=0;
    for (j=0;j<NProcs;j++){
        nimport+=mpi_nsend[ThisTask+j*NProcs];
        nexport+=mpi_nsend[j+ThisTask*NProcs];
    }
    //declare array for local storage of the appropriate size
    nlocal=nbodies-nexport+nimport;
    NImport=nimport;
    if (nexport >0) FoFGroupDataExport=new fofid_in[nexport];
    else FoFGroupDataExport=new fofid_in[1];

#ifdef MPIREDUCEMEM
    Int_t *storeval=new Int_t[nbodies];
    //if trying to reduce memory allocation,  if nlocal < than the memory allocated adjust local list so that all particles to be exported are near the end. 
    //and allocate the appropriate memory for pfof and mpi_idlist. otherwise, need to actually copy the particle data into FoFGroupDataLocal and proceed
    //as normal, storing info, send info, delete particle array, allocate a new array large enough to store info and copy over info
    ///\todo eventually I should replace arrays with vectors so that the size can change, removing the need to free and allocate
    ///\todo adjust sort type to make sure type information is kept.
    if (nlocal<Nmemlocal) {
        Noldlocal=nbodies-nexport;
        //for (i=0;i<nbodies;i++) Part[i].SetID(i);
        for (i=0;i<nbodies;i++) storeval[i]=Part[i].GetType();
        for (i=0;i<nbodies;i++) Part[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Part,nbodies,sizeof(Particle),TypeCompare);
        for (i=0;i<nbodies;i++) Part[i].SetType(storeval[Part[i].GetID()]);
        //now use array to rearrange data
        for (i=0;i<nbodies;i++) storeval[i]=mpi_foftask[Part[i].GetID()];
        for (i=0;i<nbodies;i++) mpi_foftask[i]=storeval[i];
        for (i=0;i<nbodies;i++) storeval[i]=pfof[Part[i].GetID()];
        for (i=0;i<nbodies;i++) pfof[i]=storeval[i];
        for (i=0;i<nbodies;i++) Part[i].SetID(i);
        //for sorting purposes to place untagged particles at the end. Was done by setting type
        //now via storeval and ids
        for (i=0;i<nbodies;i++) storeval[i]=-pfof[Part[i].GetID()];
        for (i=0;i<nbodies;i++) Part[i].SetID(storeval[i]);
        if (nimport>0) FoFGroupDataLocal=new fofid_in[nimport];
    }
    //otherwise use FoFGroupDataLocal to store all the necessary data
    else {
        FoFGroupDataLocal=new fofid_in[nlocal];
        for (i=0;i<nbodies;i++) storeval[i]=Part[i].GetType();
        for (i=0;i<nbodies;i++) Part[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Part,nbodies,sizeof(Particle),TypeCompare);
        for (i=0;i<nbodies;i++) Part[i].SetType(storeval[Part[i].GetID()]);
        Int_t nn=nbodies-nexport;
        for (i=0;i<nn;i++) {
            FoFGroupDataLocal[i].p=Part[i];
            FoFGroupDataLocal[i].Index = i;
            FoFGroupDataLocal[i].Task = ThisTask;
            FoFGroupDataLocal[i].iGroup = pfof[Part[i].GetID()];
        }
        //once sorted, copy info back into index order using storempival (was before via type)
        for (i=nn;i<nbodies;i++) storeval[i]=mpi_foftask[Part[i].GetID()];
        for (i=nn;i<nbodies;i++) mpi_foftask[i]=storeval[i];
        for (i=nn;i<nbodies;i++) storeval[i]=pfof[Part[i].GetID()];
        for (i=nn;i<nbodies;i++) pfof[i]=storeval[i];
        for (i=nn;i<nbodies;i++) Part[i].SetID(i);
    }
    delete[] storeval;
#endif
    //determine offsets in arrays so that data contiguous with regards to processors for broadcasting
    //offset on transmitter end
    noffset_export[0]=0;
    for (j=1;j<NProcs;j++) noffset_export[j]=noffset_export[j-1]+mpi_nsend[(j-1)+ThisTask*NProcs];
    //offset on receiver end
    for (j=0;j<NProcs;j++) {
#ifdef MPIREDUCEMEM
        if (nlocal<Nmemlocal) noffset_import[j]=0;
        else noffset_import[j]=nbodies-nexport;
#else
        noffset_import[j]=nbodies-nexport;
#endif
        if (j!=ThisTask) for (int k=0;k<j;k++)noffset_import[j]+=mpi_nsend[ThisTask+k*NProcs];
    }
    for (j=0;j<NProcs;j++) nbuffer[j]=0;
#ifdef MPIREDUCEMEM
    for (i=nbodies-nexport;i<nbodies;i++) {
        //now set particle ID to global index value,
        //note that particle PID contains global particle ID value
        //Part[i].SetID(mpi_indexlist[i]);
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting
        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Part[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfof[i];
        }
        nbuffer[task]++;
    }
#else
    FoFGroupDataLocal=new fofid_in[nlocal];
    for (i=0;i<nbodies;i++) {
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting

        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Part[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfof[i];
        }
        //otherwise, store locally
        else {
            FoFGroupDataLocal[nbuffer[task]].p=Part[i];
            FoFGroupDataLocal[nbuffer[task]].Index = i;
            FoFGroupDataLocal[nbuffer[task]].Task = ThisTask;
            FoFGroupDataLocal[nbuffer[task]].iGroup = pfof[i];
        }
        nbuffer[task]++;
    }
#endif
    //now send the data.
    ///\todo In determination of particle export for FOF routines, eventually need to place a check for the communication buffer so that if exported number 
    ///is larger than the size of the buffer, iterate over the number exported
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking send and receive from the appropriate offset point in the local export buffer 
                //to the appropriate in the receive buffer
                MPI_Sendrecv(&FoFGroupDataExport[noffset_export[recvTask]],
                    mpi_nsend[recvTask+ThisTask*NProcs] * sizeof(struct fofid_in), MPI_BYTE,
                    recvTask, TAG_FOF_C,
                    &FoFGroupDataLocal[noffset_import[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofid_in),
                    MPI_BYTE, recvTask, TAG_FOF_C, MPI_COMM_WORLD, &status);
/*                MPI_Sendrecv(&FoFGroupDataExport[noffset_export[recvTask]],
                    mpi_nsend[recvTask+ThisTask*NProcs] * sizeof(struct fofid_in), MPI_BYTE,
                    recvTask, TAG_FOF_C,
                    &FoFGroupDataLocal[noffset_import[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofid_in),
                    MPI_BYTE, recvTask, TAG_FOF_C, MPI_COMM_WORLD, &status);
                    */
            }
        }
    }
    Nlocal=nlocal;
    return nlocal;
}

/*! 
    The baryon equivalent of \ref MPIGroupExchange. Here assume baryons are searched afterwards
*/
Int_t MPIBaryonGroupExchange(const Int_t nbodies, Particle *&Part, Int_t *&pfof){
    Int_t i, j,nthreads,nexport,nimport,nlocal,n;
    Int_t nsend_local[NProcs],noffset_import[NProcs],noffset_export[NProcs],nbuffer[NProcs];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int task;
    FoFGroupDataExport=NULL;
    FoFGroupDataLocal=NULL;
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    //first determine how big a local array is needed to store linked particles and broadcast information to create new nsend array
    nlocal=0;
    for (i=0;i<nbodies;i++) {
        if (mpi_foftask[i]!=ThisTask) 
            nsend_local[mpi_foftask[i]]++;
    }
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    nexport=nimport=0;
    for (j=0;j<NProcs;j++){
        nimport+=mpi_nsend[ThisTask+j*NProcs];
        nexport+=mpi_nsend[j+ThisTask*NProcs];
    }
    //declare array for local storage of the appropriate size
    nlocal=nbodies-nexport+nimport;
    NImport=nimport;
    if (nexport >0) FoFGroupDataExport=new fofid_in[nexport];
    else FoFGroupDataExport=new fofid_in[1];

#ifdef MPIREDUCEMEM
    Nmemlocalbaryon=Nlocalbaryon[0];
    for (i=0;i<nbodies;i++) Part[i].SetID(i);
    Int_t *storeval=new Int_t[nbodies];
    //if trying to reduce memory allocation,  if nlocal < than the memory allocated adjust local list so that all particles to be exported are near the end. 
    //and allocate the appropriate memory for pfof and mpi_idlist. otherwise, need to actually copy the particle data into FoFGroupDataLocal and proceed
    //as normal, storing info, send info, delete particle array, allocate a new array large enough to store info and copy over info
    ///\todo eventually I should replace arrays with vectors so that the size can change, removing the need to free and allocate
    if (nlocal<=Nmemlocalbaryon) {
        Noldlocal=nbodies-nexport;
        for (i=0;i<nbodies;i++) storeval[i]=Part[i].GetType();
        for (i=0;i<nbodies;i++) Part[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Part,nbodies,sizeof(Particle),TypeCompare);
        for (i=0;i<nbodies;i++) Part[i].SetType(storeval[Part[i].GetID()]);
        //now use array to rearrange data
        for (i=0;i<nbodies;i++) storeval[i]=mpi_foftask[Part[i].GetID()];
        for (i=0;i<nbodies;i++) mpi_foftask[i]=storeval[i];
        for (i=0;i<nbodies;i++) storeval[i]=pfof[Part[i].GetID()];
        for (i=0;i<nbodies;i++) pfof[i]=storeval[i];
        for (i=0;i<nbodies;i++) Part[i].SetID(i);
        //now via storeval and ids
        for (i=0;i<nbodies;i++) storeval[i]=-pfof[Part[i].GetID()];
        for (i=0;i<nbodies;i++) Part[i].SetID(storeval[i]);
        if (nimport>0) FoFGroupDataLocal=new fofid_in[nimport];
    }
    //otherwise use FoFGroupDataLocal to store all the necessary data
    else {
        FoFGroupDataLocal=new fofid_in[nlocal];
        for (i=0;i<nbodies;i++) storeval[i]=Part[i].GetType();
        for (i=0;i<nbodies;i++) Part[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Part,nbodies,sizeof(Particle),TypeCompare);
        for (i=0;i<nbodies;i++) Part[i].SetType(storeval[Part[i].GetID()]);
        Int_t nn=nbodies-nexport;
        for (i=0;i<nn;i++) {
            FoFGroupDataLocal[i].p=Part[i];
            FoFGroupDataLocal[i].Index = i;
            FoFGroupDataLocal[i].Task = ThisTask;
            FoFGroupDataLocal[i].iGroup = pfof[Part[i].GetID()];
        }
        for (i=nn;i<nbodies;i++) storeval[i]=mpi_foftask[Part[i].GetID()];
        for (i=nn;i<nbodies;i++) mpi_foftask[i]=storeval[i];
        for (i=nn;i<nbodies;i++) storeval[i]=pfof[Part[i].GetID()];
        for (i=nn;i<nbodies;i++) pfof[i]=storeval[i];
        for (i=nn;i<nbodies;i++) Part[i].SetID(i);
    }
    delete[] storeval;
#endif
    //determine offsets in arrays so that data contiguous with regards to processors for broadcasting
    //offset on transmitter end
    noffset_export[0]=0;
    for (j=1;j<NProcs;j++) noffset_export[j]=noffset_export[j-1]+mpi_nsend[(j-1)+ThisTask*NProcs];
    //offset on receiver end
    for (j=0;j<NProcs;j++) {
#ifdef MPIREDUCEMEM
        if (nlocal<Nlocalbaryon[0]) noffset_import[j]=0;
        else noffset_import[j]=nbodies-nexport;
#else
        noffset_import[j]=nbodies-nexport;
#endif
        if (j!=ThisTask) for (int k=0;k<j;k++)noffset_import[j]+=mpi_nsend[ThisTask+k*NProcs];
    }
    for (j=0;j<NProcs;j++) nbuffer[j]=0;
#ifdef MPIREDUCEMEM
    for (i=nbodies-nexport;i<nbodies;i++) {
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting
        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Part[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfof[i];
        }
        nbuffer[task]++;
    }
#else
    FoFGroupDataLocal=new fofid_in[nlocal];
    for (i=0;i<nbodies;i++) {
        //now set particle ID to global index value,
        //note that particle PID contains global particle ID value
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting

        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Part[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfof[i];
        }
        //otherwise, store locally
        else {
            FoFGroupDataLocal[nbuffer[task]].p=Part[i];
            FoFGroupDataLocal[nbuffer[task]].Index = i;
            FoFGroupDataLocal[nbuffer[task]].Task = ThisTask;
            FoFGroupDataLocal[nbuffer[task]].iGroup = pfof[i];
        }
        nbuffer[task]++;
    }
#endif
    //now send the data.
    ///\todo In determination of particle export for FOF routines, eventually need to place a check for the communication buffer so that if exported number 
    ///is larger than the size of the buffer, iterate over the number exported
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking send and receive from the appropriate offset point in the local export buffer 
                //to the appropriate in the receive buffer
                MPI_Sendrecv(&FoFGroupDataExport[noffset_export[recvTask]],
                    mpi_nsend[recvTask+ThisTask*NProcs] * sizeof(struct fofid_in), MPI_BYTE,
                    recvTask, TAG_FOF_C,
                    &FoFGroupDataLocal[noffset_import[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofid_in),
                    MPI_BYTE, recvTask, TAG_FOF_C, MPI_COMM_WORLD, &status);
            }
        }
    }
    Nlocalbaryon[0]=nlocal;
    return nlocal;
}

///Determine the local number of groups and their sizes (groups must be local to an mpi thread)
Int_t MPICompileGroups(const Int_t nbodies, Particle *&Part, Int_t *&pfof, Int_t minsize){
    Int_t i,j,start,ngroups;
    Int_t *numingroup,*groupid,**plist;
    ngroups=0;
    //if minimizing memory load when using mpi (by adding extra routines to determine memory required) 
    //first check to see if local memory is enough to contained expected number of particles
#ifdef MPIREDUCEMEM
    //if local mem is enough, copy data from the FoFGroupDataLocal 
    if(Nmemlocal>nbodies) {
        for (i=Noldlocal;i<nbodies;i++) {
            Part[i]=FoFGroupDataLocal[i-Noldlocal].p;
            //note that before used type to sort particles 
            //Part[i].SetID(i);
            //Part[i].SetType(-FoFGroupDataLocal[i-Noldlocal].iGroup);
            //now use id
            Part[i].SetID(-FoFGroupDataLocal[i-Noldlocal].iGroup);
        }
        //used to use ID store store group id info
        qsort(Part,nbodies,sizeof(Particle),IDCompare);
        //determine the # of groups, their size and the current group ID
        for (i=0,start=0;i<nbodies;i++) {
            if (Part[i].GetID()!=Part[start].GetID()) {
                //if group is too small set type to zero, which currently is used to store the group id
                if ((i-start)<minsize) for (Int_t j=start;j<i;j++) Part[j].SetID(0);
                else ngroups++;
                start=i;
            }
            if (Part[i].GetID()==0) break;
        }
        //again resort to move untagged particles to the end. 
        qsort(Part,nbodies,sizeof(Particle),IDCompare);
        //now adjust pfof and ids.
        for (i=0;i<nbodies;i++) {pfof[i]=-Part[i].GetID();Part[i].SetID(i);}
        numingroup=new Int_t[ngroups+1];
        plist=new Int_t*[ngroups+1];
        ngroups=1;//offset as group zero is untagged
        for (i=0,start=0;i<nbodies;i++) {
            if (pfof[i]!=pfof[start]) {
                numingroup[ngroups]=i-start;
                plist[ngroups]=new Int_t[numingroup[ngroups]];
                for (Int_t j=start,count=0;j<i;j++) plist[ngroups][count++]=j;
                ngroups++;
                start=i;
            }
            if (pfof[i]==0) break;
        }
        ngroups--;
        //for (i=0;i<nbodies;i++) mpi_idlist[i]=Part[i].GetPID();
    }
    else {
#endif
    //sort local list
    qsort(FoFGroupDataLocal, nbodies, sizeof(struct fofid_in), fof_id_cmp);
    //determine the # of groups, their size and the current group ID
    for (i=0,start=0;i<nbodies;i++) {
        if (FoFGroupDataLocal[i].iGroup!=FoFGroupDataLocal[start].iGroup) {
            if ((i-start)<minsize){
                for (Int_t j=start;j<i;j++) FoFGroupDataLocal[j].iGroup=0;
            }
            else ngroups++;
            start=i;
        }
        if (FoFGroupDataLocal[i].iGroup==0) break;
    }
    //now sort again which will put particles group then id order, and determine size of groups and their current group id;
    qsort(FoFGroupDataLocal, nbodies, sizeof(struct fofid_in), fof_id_cmp);
    numingroup=new Int_t[ngroups+1];
    plist=new Int_t*[ngroups+1];
    ngroups=1;//offset as group zero is untagged
    for (i=0,start=0;i<nbodies;i++) {
        if (FoFGroupDataLocal[i].iGroup!=FoFGroupDataLocal[start].iGroup) {
            numingroup[ngroups]=i-start;
            plist[ngroups]=new Int_t[numingroup[ngroups]];
            for (Int_t j=start,count=0;j<i;j++) plist[ngroups][count++]=j;
            ngroups++;
            start=i;
        }
        if (FoFGroupDataLocal[i].iGroup==0) break;
    }
    ngroups--;
    for (i=0;i<nbodies;i++) pfof[i]=FoFGroupDataLocal[i].iGroup;
    //and store the particles global ids
    for (i=0;i<nbodies;i++) {
        Part[i]=FoFGroupDataLocal[i].p;
        Part[i].SetID(i);
        //mpi_idlist[i]=FoFGroupDataLocal[i].p.GetPID();
    }
#ifdef MPIREDUCEMEM
    }
#endif
    //reorder groups ids according to size
    ReorderGroupIDs(ngroups,ngroups,numingroup,pfof,plist);
    for (i=1;i<=ngroups;i++) delete[] plist[i];
    delete[] plist;
    delete[] numingroup;
    //broadcast number of groups so that ids can be properly offset
    MPI_Allgather(&ngroups, 1, MPI_Int_t, mpi_ngroups, 1, MPI_Int_t, MPI_COMM_WORLD);
    if(FoFGroupDataLocal!=NULL) delete[] FoFGroupDataLocal;
    if(FoFGroupDataExport!=NULL) delete[] FoFGroupDataExport;
    return ngroups;
}

///Similar to \ref MPICompileGroups but optimised for separate baryon search
Int_t MPIBaryonCompileGroups(const Int_t nbodies, Particle *&Part, Int_t *&pfof, Int_t minsize, int iorder){
    Int_t i,j,start,ngroups;
    Int_t *numingroup,*groupid,**plist;
    ngroups=0;

    //if minimizing memory load when using mpi (by adding extra routines to determine memory required) 
    //first check to see if local memory is enough to contained expected number of particles
#ifdef MPIREDUCEMEM
    //if local mem is enough, copy data from the FoFGroupDataLocal 
    if(Nmemlocalbaryon>nbodies) {
        for (i=Noldlocal;i<nbodies;i++) {
            Part[i]=FoFGroupDataLocal[i-Noldlocal].p;
            Part[i].SetID(-FoFGroupDataLocal[i-Noldlocal].iGroup);
        }
        //now use ID
        qsort(Part,nbodies,sizeof(Particle),IDCompare);
        //determine the # of groups, their size and the current group ID
        for (i=0,start=0;i<nbodies;i++) {
            if (Part[i].GetID()!=Part[start].GetID()) {
                //if group is too small set type to zero, which currently is used to store the group id
                if ((i-start)<minsize) for (Int_t j=start;j<i;j++) Part[j].SetID(0);
                else ngroups++;
                start=i;
            }
            if (Part[i].GetID()==0) break;
        }

        //again resort to move untagged particles to the end. 
        qsort(Part,nbodies,sizeof(Particle),IDCompare);
        //now adjust pfof and ids.
        for (i=0;i<nbodies;i++) {pfof[i]=-Part[i].GetID();Part[i].SetID(i);}
        numingroup=new Int_t[ngroups+1];
        plist=new Int_t*[ngroups+1];
        ngroups=1;//offset as group zero is untagged
        for (i=0,start=0;i<nbodies;i++) {
            if (pfof[i]!=pfof[start]) {
                numingroup[ngroups]=i-start;
                plist[ngroups]=new Int_t[numingroup[ngroups]];
                for (Int_t j=start,count=0;j<i;j++) plist[ngroups][count++]=j;
                ngroups++;
                start=i;
            }
            if (pfof[i]==0) break;
        }
        ngroups--;
    }
    else {
#endif
    //sort local list
    qsort(FoFGroupDataLocal, nbodies, sizeof(struct fofid_in), fof_id_cmp);
    //determine the # of groups, their size and the current group ID
    for (i=0,start=0;i<nbodies;i++) {
        if (FoFGroupDataLocal[i].iGroup!=FoFGroupDataLocal[start].iGroup) {
            if ((i-start)<minsize){
                for (Int_t j=start;j<i;j++) FoFGroupDataLocal[j].iGroup=0;
            }
            else ngroups++;
            start=i;
        }
        if (FoFGroupDataLocal[i].iGroup==0) break;
    }
    //now sort again which will put particles group then id order, and determine size of groups and their current group id;
    qsort(FoFGroupDataLocal, nbodies, sizeof(struct fofid_in), fof_id_cmp);
    numingroup=new Int_t[ngroups+1];
    plist=new Int_t*[ngroups+1];
    ngroups=1;//offset as group zero is untagged
    for (i=0,start=0;i<nbodies;i++) {
        if (FoFGroupDataLocal[i].iGroup!=FoFGroupDataLocal[start].iGroup) {
            numingroup[ngroups]=i-start;
            plist[ngroups]=new Int_t[numingroup[ngroups]];
            for (Int_t j=start,count=0;j<i;j++) plist[ngroups][count++]=j;
            ngroups++;
            start=i;
        }
        if (FoFGroupDataLocal[i].iGroup==0) break;
    }
    ngroups--;
    for (i=0;i<nbodies;i++) pfof[i]=FoFGroupDataLocal[i].iGroup;
    //and store the particles global ids
    for (i=0;i<nbodies;i++) {
        Part[i]=FoFGroupDataLocal[i].p;
        Part[i].SetID(i);
    }
#ifdef MPIREDUCEMEM
    }
#endif
    //reorder groups ids according to size if required.
    if (iorder) ReorderGroupIDs(ngroups,ngroups,numingroup,pfof,plist);
    for (i=1;i<=ngroups;i++) if (numingroup[i]>0) delete[] plist[i];
    delete[] plist;
    delete[] numingroup;

    //broadcast number of groups so that ids can be properly offset
    MPI_Allgather(&ngroups, 1, MPI_Int_t, mpi_ngroups, 1, MPI_Int_t, MPI_COMM_WORLD);
    if(FoFGroupDataLocal!=NULL) delete[] FoFGroupDataLocal;
    if(FoFGroupDataExport!=NULL) delete[] FoFGroupDataExport;
    return ngroups;
}

///Determine which exported dm particle is closest in phase-space to a local baryon particle and assign that particle to the group of that dark matter particle if is closest particle
Int_t MPISearchBaryons(const Int_t nbaryons, Particle *&Pbaryons, Int_t *&pfofbaryons, Int_t *numingroup, Double_t *localdist, Int_t nsearch, Double_t *param, Double_t *period)
{
    Double_t D2, dval, rval;
    Coordinate x1; 
    Particle p1;
    Int_t  i, j, k, pindex,nexport=0;
    int tid;
    FOFcompfunc fofcmp=FOF6d;
    Int_t *nnID;
    Double_t *dist2;
    if (NImport>0) {
    //now dark matter particles associated with a group existing on another mpi domain are local and can be searched. 
    KDTree *mpitree=new KDTree(PartDataGet,NImport,nsearch/2,mpitree->TPHYS,mpitree->KEPAN,100,0,0,0,period);
    if (nsearch>NImport) nsearch=NImport;
#ifdef USEOPENMP
#pragma omp parallel default(shared) \
private(i,j,k,tid,p1,pindex,x1,D2,dval,rval,nnID,dist2)
{
    nnID=new Int_t[nsearch];
    dist2=new Double_t[nsearch];
#pragma omp for reduction(+:nexport)
#endif
    for (i=0;i<nbaryons;i++)
    {
#ifdef USEOPENMP
        tid=omp_get_thread_num();
#else
        tid=0;
#endif
        p1=Pbaryons[i];
        x1=Coordinate(p1.GetPosition());
        rval=MAXVALUE;
        dval=localdist[i];
        mpitree->FindNearestPos(x1, nnID, dist2,nsearch);
        if (dist2[0]<param[6]) {
        for (j=0;j<nsearch;j++) {
            D2=0;
            pindex=PartDataGet[nnID[j]].GetID();
            if (numingroup[pfofbaryons[i]]<FoFDataGet[pindex].iLen) {
                if (fofcmp(p1,PartDataGet[nnID[j]],param)) {
                    for (k=0;k<3;k++) {
                        D2+=(p1.GetPosition(k)-PartDataGet[nnID[j]].GetPosition(k))*(p1.GetPosition(k)-PartDataGet[nnID[j]].GetPosition(k))/param[6]+(p1.GetVelocity(k)-PartDataGet[nnID[j]].GetVelocity(k))*(p1.GetVelocity(k)-PartDataGet[nnID[j]].GetVelocity(k))/param[7];
                    }
#ifdef GASON
                    D2+=p1.GetU()/param[7];
#endif
                    if (dval>D2) {dval=D2;pfofbaryons[i]=FoFDataGet[pindex].iGroup;rval=dist2[j];mpi_foftask[i]=FoFDataGet[pindex].iGroupTask;}
                }
            }
        }
        }
        nexport+=(mpi_foftask[i]!=ThisTask);
    }
    delete[] nnID;
    delete[] dist2;
#ifdef USEOPENMP
}
#endif 
    }
    return nexport;
}

Int_t MPIBaryonExchange(const Int_t nbaryons, Particle *Pbaryons, Int_t *pfofbaryons){
    Int_t i, j,nthreads,nexport,nimport,nlocal,n;
    Int_t nsend_local[NProcs],noffset_import[NProcs],noffset_export[NProcs],nbuffer[NProcs];
    Int_t sendTask,recvTask;
    MPI_Status status;
    int task;
    //initial containers to send info across threads
    FoFGroupDataExport=NULL;
    FoFGroupDataLocal=NULL;

    MPI_Barrier(MPI_COMM_WORLD);
    //first determine how big a local array is needed to store tagged baryonic particles
    for (j=0;j<NProcs;j++) nsend_local[j]=0;
    nlocal=0;
    for (i=0;i<nbaryons;i++) {
        if (mpi_foftask[i]!=ThisTask) 
            nsend_local[mpi_foftask[i]]++;
    }
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    nexport=nimport=0;
    for (j=0;j<NProcs;j++){
        nimport+=mpi_nsend[ThisTask+j*NProcs];
        nexport+=mpi_nsend[j+ThisTask*NProcs];
    }
    //declare array for local storage of the appropriate size
    nlocal=nbaryons-nexport+nimport;
    //store import number
    NImport=nimport;
    FoFGroupDataExport=new fofid_in[nexport+1];//+1 just buffer to ensure that if nothing broadcast, easy to allocate and deallocate memory

    //if trying to reduce memory allocation, then need to check amount stored locally and how much that needs to be adjusted by 
    //if nlocal < than the memory allocated adjust local list so that all particles to be exported are near the end. 
    //and allocate the appropriate memory for pfofbaryons and mpi_idlist. otherwise, need to actually copy the particle data into FoFGroupDataLocal and proceed
    //as normal, storing info, send info, delete particle array, allocate a new array large enough to store info and copy over info
    ///\todo eventually I should replace arrays with vectors so that the size can change, removing the need to free and allocate
#ifdef MPIREDUCEMEM
    Int_t *storeval=new Int_t[nbaryons];
    if (nlocal<Nmemlocal) {
        Noldlocal=nbaryons-nexport;
        for (i=0;i<nbaryons;i++) storeval[i]=Pbaryons[i].GetType();
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Pbaryons,nbaryons,sizeof(Particle),TypeCompare);
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType(storeval[Pbaryons[i].GetID()]);
        //now use array to rearrange data
        for (i=0;i<nbaryons;i++) storeval[i]=mpi_foftask[Pbaryons[i].GetID()];
        for (i=0;i<nbaryons;i++) mpi_foftask[i]=storeval[i];
        for (i=0;i<nbaryons;i++) storeval[i]=pfofbaryons[Pbaryons[i].GetID()];
        for (i=0;i<nbaryons;i++) pfofbaryons[i]=storeval[i];
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetID(i);
        //for sorting purposes to place untagged particles at the end. 
        for (i=0;i<nbaryons;i++) storeval[i]=-pfofbaryons[Pbaryons[i].GetID()];
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetID(storeval[i]);
        if (nimport>0) FoFGroupDataLocal=new fofid_in[nimport];
    }
    //otherwise use FoFGroupDataLocal to store all the necessary data
    else {
        FoFGroupDataLocal=new fofid_in[nlocal];
        for (i=0;i<nbaryons;i++) storeval[i]=Pbaryons[i].GetType();
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType((mpi_foftask[i]!=ThisTask));
        qsort(Pbaryons,nbaryons,sizeof(Particle),TypeCompare);
        for (i=0;i<nbaryons;i++) Pbaryons[i].SetType(storeval[Pbaryons[i].GetID()]);
        Int_t nn=nbaryons-nexport;
        for (i=0;i<nn;i++) {
            FoFGroupDataLocal[i].p=Pbaryons[i];
            FoFGroupDataLocal[i].Index = i;
            FoFGroupDataLocal[i].Task = ThisTask;
            FoFGroupDataLocal[i].iGroup = pfofbaryons[Pbaryons[i].GetID()];
        }
        for (i=nn;i<nbaryons;i++) storeval[i]=mpi_foftask[Pbaryons[i].GetID()];
        for (i=nn;i<nbaryons;i++) mpi_foftask[i]=storeval[i];
        for (i=nn;i<nbaryons;i++) storeval[i]=pfofbaryons[Pbaryons[i].GetID()];
        for (i=nn;i<nbaryons;i++) pfofbaryons[i]=storeval[i];
        for (i=nn;i<nbaryons;i++) Pbaryons[i].SetID(i);
    }
    delete[] storeval;
#else
    FoFGroupDataLocal=new fofid_in[nlocal];
#endif

    //determine offsets in arrays so that data contiguous with regards to processors for broadcasting
    //offset on transmitter end
    noffset_export[0]=0;
    for (j=1;j<NProcs;j++) noffset_export[j]=noffset_export[j-1]+mpi_nsend[(j-1)+ThisTask*NProcs];
    for (j=0;j<NProcs;j++) {
#ifdef MPIREDUCEMEM
        if (nlocal<Nmemlocal) noffset_import[j]=0;
        else noffset_import[j]=nbaryons-nexport;
#else
        noffset_import[j]=nbaryons-nexport;
#endif
        if (j!=ThisTask) for (int k=0;k<j;k++)noffset_import[j]+=mpi_nsend[ThisTask+k*NProcs];
    }
    for (j=0;j<NProcs;j++) nbuffer[j]=0;
#ifdef MPIREDUCEMEM
    for (i=nbaryons-nexport;i<nbaryons;i++) {
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting
        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Pbaryons[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfofbaryons[i];
        }
        nbuffer[task]++;
    }
#else
    for (i=0;i<nbaryons;i++) {
        //now set particle ID to global index value,
        //note that particle PID contains global particle ID value
        Pbaryons[i].SetID(mpi_indexlist[i]);
        //if particle belongs to group that should belong on a different mpi thread, store for broadcasting

        task=mpi_foftask[i];
        if (task!=ThisTask) {
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].p=Pbaryons[i];
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Index = i;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].Task = task;
            FoFGroupDataExport[noffset_export[task]+nbuffer[task]].iGroup = pfofbaryons[i];
        }
        //otherwise, store locally
        else {
            FoFGroupDataLocal[nbuffer[task]].p=Pbaryons[i];
            FoFGroupDataLocal[nbuffer[task]].Index = i;
            FoFGroupDataLocal[nbuffer[task]].Task = ThisTask;
            FoFGroupDataLocal[nbuffer[task]].iGroup = pfofbaryons[i];
        }
        nbuffer[task]++;
    }
#endif
    //now send the data.
    ///\todo In determination of particle export for FOF routines, eventually need to place a check for the communication buffer so that if exported number 
    ///is larger than the size of the buffer, iterate over the number exported
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;
            if(mpi_nsend[ThisTask * NProcs + recvTask] > 0 || mpi_nsend[recvTask * NProcs + ThisTask] > 0)
            {
                //blocking send and receive from the appropriate offset point in the local export buffer 
                //to the appropriate in the receive buffer
                MPI_Sendrecv(&FoFGroupDataExport[noffset_export[recvTask]],
                    mpi_nsend[recvTask+ThisTask*NProcs] * sizeof(struct fofid_in), MPI_BYTE,
                    recvTask, TAG_FOF_C,
                    &FoFGroupDataLocal[noffset_import[recvTask]],
                    mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct fofid_in),
                    MPI_BYTE, recvTask, TAG_FOF_C, MPI_COMM_WORLD, &status);
            }
        }
    }

    Nlocalbaryon[0]=nlocal;
    return nlocal;
}
//@}


/// \name FOF routines related to modifying group ids
//@{

///This alters the group ids by an offset determined by the number of groups on all previous mpi threads so that the group 
///has a unique group id. Prior to this, group ids are determined locally.
inline void MPIAdjustGroupIDs(const Int_t nbodies,Int_t *pfof) {
    Int_t noffset=0;
    for (int j=0;j<ThisTask;j++)noffset+=mpi_ngroups[j];
    for (Int_t i=0;i<nbodies;i++) if (pfof[i]>0) pfof[i]+=noffset;
}

///Collect FOF from all 
void MPICollectFOF(const Int_t nbodies, Int_t *&pfof){
    Int_t sendTask,recvTask;
    MPI_Status status;
    //if using mpi, offset the pfof so that group ids are no unique, before just local to thread
    MPIAdjustGroupIDs(Nlocal,pfof);
    //now send the data from all MPI threads to thread zero
    //first must send how much data is local to a processor
    Int_t nsend_local[NProcs];
    for (int i=0;i<NProcs;i++) nsend_local[i]=0;
    if (ThisTask!=0)nsend_local[0]=Nlocal;
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    recvTask=0;
    //next copy task zero pfof into global mpi_pfof to the appropriate indices
    if (ThisTask==0) {
        for (Int_t i=0;i<Nlocal;i++) mpi_pfof[mpi_indexlist[i]]=pfof[i];
    }
    //then for each processor copy their values into local pfof and mpi_indexlist
    //note mpi_idlist contains id values whereas indexlist contains the index order of how particles were loaded
    //this requires determining the largest size needed
    if (ThisTask==0) {
        Int_t maxnlocal=0;
        for(int j=1;j<NProcs;j++) if (maxnlocal<mpi_nsend[j*NProcs]) maxnlocal=mpi_nsend[j*NProcs];
        delete[] pfof;
        delete[] mpi_indexlist;
        pfof=new Int_t[maxnlocal];
        mpi_indexlist=new Int_t[maxnlocal];
    }
    MPI_Barrier(MPI_COMM_WORLD);
    //now for each mpi task, copy appropriate data to mpi thread 0 local buffers 
    for(int j=1;j<NProcs;j++)
    {
        sendTask=j;
        recvTask=0;
        if (ThisTask==sendTask) {
            MPI_Ssend(pfof, Nlocal , MPI_Int_t,0, TAG_FOF_D, MPI_COMM_WORLD);
            MPI_Ssend(mpi_indexlist, Nlocal , MPI_Int_t,0, TAG_FOF_E, MPI_COMM_WORLD);
        }
        if(ThisTask==0) {
            MPI_Recv(pfof, mpi_nsend[sendTask*NProcs], MPI_Int_t, sendTask, TAG_FOF_D, MPI_COMM_WORLD, &status);
            MPI_Recv(mpi_indexlist, mpi_nsend[sendTask*NProcs], MPI_Int_t, sendTask, TAG_FOF_E, MPI_COMM_WORLD, &status);
            for (Int_t i=0;i<mpi_nsend[sendTask*NProcs];i++) mpi_pfof[mpi_indexlist[i]]=pfof[i];
        }
        MPI_Barrier(MPI_COMM_WORLD);
    }
}
//@}

/// \name Routines related to distributing the grid cells used to calculate the coarse-grained mean field
//@{
/*! Collects all the grid data
*/
void MPIBuildGridData(const Int_t ngrid, GridCell *grid, Coordinate *gvel, Matrix *gveldisp){
    Int_t i, j,nthreads,nexport=0;
    Int_t nsend_local[NProcs],noffset[NProcs],nbuffer[NProcs];
    Int_t sendTask,recvTask;
    MPI_Status status;

    for (j=0;j<NProcs;j++) nsend_local[j]=Ngridlocal;
    MPI_Allgather(nsend_local, NProcs, MPI_Int_t, mpi_nsend, NProcs, MPI_Int_t, MPI_COMM_WORLD);
    noffset[0]=0;
    for (j=1;j<NProcs;j++) noffset[j]=noffset[j]+mpi_nsend[ThisTask+j*NProcs];
    for (i=0;i<Ngridlocal;i++) {
        for (j=0;j<3;j++) mpi_grid[noffset[ThisTask]+i].xm[j]=grid[i].xm[j];
        mpi_gvel[noffset[ThisTask]+i]=gvel[i];
        mpi_gveldisp[noffset[ThisTask]+i]=gveldisp[i];
    }
    
    for(j=0;j<NProcs;j++)
    {
        if (j!=ThisTask)
        {
            sendTask = ThisTask;
            recvTask = j;//ThisTask^j;//bitwise XOR ensures that recvTask cycles around sendTask
            //blocking point-to-point send and receive. 
            MPI_Sendrecv(grid,
                Ngridlocal* sizeof(struct GridCell), MPI_BYTE,
                recvTask, TAG_GRID_A,
                &mpi_grid[noffset[recvTask]],
                mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct GridCell),
                MPI_BYTE, recvTask, TAG_GRID_A, MPI_COMM_WORLD, &status);
            MPI_Sendrecv(gvel,
                Ngridlocal* sizeof(struct Coordinate), MPI_BYTE,
                recvTask, TAG_GRID_B,
                &mpi_gvel[noffset[recvTask]],
                mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct Coordinate),
                MPI_BYTE, recvTask, TAG_GRID_B, MPI_COMM_WORLD, &status);
            MPI_Sendrecv(gveldisp,
                Ngridlocal* sizeof(struct Matrix), MPI_BYTE,
                recvTask, TAG_GRID_C,
                &mpi_gveldisp[noffset[recvTask]],
                mpi_nsend[ThisTask+recvTask * NProcs] * sizeof(struct Matrix),
                MPI_BYTE, recvTask, TAG_GRID_C, MPI_COMM_WORLD, &status);
        }
    }
}
//@}

/// \name comparison functions used to assign particles to a specific mpi thread
//@{
///comprasion function used to sort particles for export so that all particles being exported to the same processor are in a contiguous block and well ordered
///\todo, I should remove this and determine the ordering before hand.
int fof_export_cmp(const void *a, const void *b)
{
  if(((struct fofdata_in *) a)->Task < (((struct fofdata_in *) b)->Task))
    return -1;
  if(((struct fofdata_in *) a)->Task > (((struct fofdata_in *) b)->Task))
    return +1;
  return 0;
}

///comprasion function used to sort particles for export so that all particles being exported to the same processor are in a contiguous block and well ordered
int nn_export_cmp(const void *a, const void *b)
{
  if(((struct nndata_in *) a)->ToTask < (((struct nndata_in *) b)->ToTask))
    return -1;
  if(((struct nndata_in *) a)->ToTask > (((struct nndata_in *) b)->ToTask))
    return +1;
  return 0;
}

///comprasion function used to sort grouped particles so that easy to determine total number of groups locally, size of groups, etc.
int fof_id_cmp(const void *a, const void *b)
{
  if(((struct fofid_in *) a)->iGroup > (((struct fofid_in *) b)->iGroup))
    return -1;

  if(((struct fofid_in *) a)->iGroup < (((struct fofid_in *) b)->iGroup))
    return +1;

  if(((struct fofid_in *) a)->p.GetType() < (((struct fofid_in *) b)->p.GetType()))
    return -1;

  if(((struct fofid_in *) a)->p.GetType() > (((struct fofid_in *) b)->p.GetType()))
    return +1;


  if(((struct fofid_in *) a)->p.GetID() < (((struct fofid_in *) b)->p.GetID()))
    return -1;

  if(((struct fofid_in *) a)->p.GetID() > (((struct fofid_in *) b)->p.GetID()))
    return +1;

  return 0;
}
//@}

#endif

